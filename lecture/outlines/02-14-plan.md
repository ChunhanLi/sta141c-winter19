Announcements:

- I'll hold a little extra office hours after class today. Siteng has office hours tomorrow, so I don't think it's necessary to be up at night.
- Video is uploaded
- Scores are available for half of participation and hw3
- Project proposals due next week, today is a good time to find partners- think about what tech you want to use.


## Project

Use any technology you want.

Let's discuss- what are some interesting questions we could ask given this data set?

Who is dying to use particular technologies?
Raise hands, and look around, so that you can form groups based on mutual interest.


## Review

[Anti patterns](https://en.wikipedia.org/wiki/Anti-pattern#Software_engineering)

This code has a problem: it unnecessarily uses a global variable `y`.

```{r}
fbad = function(x) x + y

fgood = function(x, y) x + y
```

The codetools package has tools to help check your code.

```{r}
# Shows we have a global variable
codetools::checkUsage(fbad)

# No messages
codetools::checkUsage(fgood)
```

Also: lintr package in R. 


## Shared Memory Parallel Processing

Terms:

- __process__ A single running program (here an instance of the R interpreter)
- __fork__ When the operating system duplicates a process
- __parent__ The original process
- __child__ The newly created process

Show processes in activity monitor.
My Mac has 581 processes running a total of 1966 threads as I type this.

The operating system can `fork` a process.

Make a large object.

```{r}
big = rnorm(1e8)

print(object.size(big), units = "MB")
```

In the activity monitor we see a single R process taking up a bunch of memory.

```{r}
library(parallel)

cls = makeCluster(2L, type = "FORK")
```

Now there are 2 children R processes.

```{r}
Sys.getpid()

clusterCall(cls, Sys.getpid)
```

We can access the big object without copying it.

```{r}
clusterEvalQ(cls, big[1])
```

Child processes using minimal memory.


### Copy on Write

The children get a copy of the parent process at the moment that the child is created.

State is shared only  until the parents or the children change them.
The changes are 'copy on write', which are R's normal semantics.

```{r}
clusterEvalQ(cls, big[1] <- 100)
```

Now each of the children seem to be using 1.5 GB.
Question: What is going on?
Answer: This is about twice the size of the object.
Which means R copied the object once from the parent to the child, and then copied it again in the child when it made the change.
Eventually the spare copy will be garbage collected, and R can release memory back to the OS.

```{r}
clusterEvalQ(cls, gc())
```

### More Convenient Options

We can simply swap in `parallel::mclapply` for `lapply`:

```{r}
x = 1:5

y = lapply(x, seq)

y2 = parallel::mclapply(x, seq, mc.cores = 2L)
```

`parallel::mclapply` does the equivalent of:

- set up a local cluster with `mc.cores` workers
- distribute the `lapply` across workers
- receive the results
- shut down and remove the cluster

If you just want to run a single command asynchronously (in the background) in parallel you can do that with `mcparallel` followed by `mccollect`.

```{r}
job = mcparallel(1:5)

# ... do other stuff...

result = mccollect(job)
```


### Side note

We can use the command line and pipes to control the computer.

For example, to kill the children processes from the command line:

```{bash}
$ pgrep -P 8261 | xargs kill
```

Here's what's going on in this command:

- `pgrep -P 8261` prints the processes that are children of process 8261 to `stdout`.
- `kill 1234` will terminate process 1234. The `kill` command accepts process id's as arguments, but it cannot read them from `stdin`.
- `xargs` takes `stdin` and makes it appear as an argument that we explicitly passed.

`xargs` is another example of the flexibility and utility of the UNIX philosophy.


## Memory mapping

We can also treat the disk as if it were memory.

+ objects larger than memory
- slower than memory

```{r}
library(bigmemory)

m = filebacked.big.matrix(10, 10
    , backingfile = "m.bin"
    , descriptorfile = "m.bin.desc"
)
```

We can make changes if we need to:

```{r}
m[1, 1] = 100

m[1, 1]

# Write the changes to disk.
flush(m)
```

Then, from another R process we can call: 

```{r}
m = attach.big.matrix("m.bin.desc")

m[1, 1]

m[1, 1] = 200
```

The other processes pick up the result.

There are other solutions out there - hdf5 comes to mind.
