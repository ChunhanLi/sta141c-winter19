## Announcements

- We'll post solutions to HW5, but don't rely on them to do the peer review, because there are many possible valid solutions.
- I have a new SQLite database built for you.
    I'm in the process of zipping and transferring it.
    I plan to post it later today.
- Regarding grading- Homework 5 was harder, but worth less points on Canvas than the others.
    Homeworks 1-5 will all be worth the same amount, and Homework 6 will be worth 1/4 of that amount.
    We'll drop the lowest of homeworks 1-6.


## Project notes

- It can be frustrating and overwhelming to work with large data sets.
    They're rarely in the form you want, they're incomplete or have duplicates, and we often don't know exactly what things mean.
- You'll probably get going faster by starting small.
    Start by filtering the rows and columns to something more manageable than the entire data set.
- Remember to address specific, focused questions

Do the best with what you have.
When in doubt, use common sense and state your assumptions.
For example:

> We noted that the annual totals for `total_funding` in the `awards` table are less than the aggregate figures provided by the Congressional Budget Office (CBO).
> However, since 2008, the numbers consistently aggregate to about 80% of those of the CBO, so we've restricted our analysis to this time period.



## Review

A good analogy for performance improvement:
I filled up the disk on my local hard drive while doing this class.
To make more space I went through and deleted some of the largest files I had.
Tuning a program for performance is the same way- we want to eliminate the largest bottlenecks.


Here's some industrial grade SQL.
This is how the `universal_transaction_matview` table gets made, which is the same as 
I didn't write this query- rather, it's how the Postgres database was designed.
It's the materialized view of the data that powers most of the usaspending website.

A few interesting notes:
- `to_tsvector` are used to create a text searching vector
- `COALESCE` is a SQL function that returns the first non `NULL` argument
- `CASE WHEN` offers a simple form of conditional expression
- `obligation_to_enum` is a user defined function that does binning
- `transaction_normalized LEFT JOIN ...` does a left join on a whopping __19__ different tables.
    This essentially means taking the `transaction_normalized` table and adding much more information to it.



## Hadoop

The idea is that we have many (1000's) of compute node working on the same problem.
The usaspending data we are working with now could possibly benefit from this.

Let's login to a hadoop cluster and look at some distributed files:

```{bash}
~ ssh hadoop

clarkf@hadoop ~ $ hdfs dfs -ls
Found 11 items
drwx------   - clarkf hdfs          0 2017-11-21 16:00 .Trash
drwxr-xr-x   - clarkf hdfs          0 2017-06-21 16:25 .hiveJars
drwxr-xr-x   - clarkf hdfs          0 2017-10-27 15:30 fundamental_diagram
drwxr-xr-x   - clarkf hdfs          0 2017-06-22 17:17 pems
drwxr-xr-x   - clarkf hdfs          0 2017-07-20 11:37 pems_parquet
...
```

These are directories containing conceptual "chunks" of files.

The `pems` directories contains some large data files:

```{bash}
$ hdfs dfs -du -h pems
69.7 M   pems/d04_text_station_raw_2016_01_01.txt.gz
77.9 M   pems/d04_text_station_raw_2016_01_02.txt.gz
67.5 M   pems/d04_text_station_raw_2016_01_03.txt.gz
78.7 M   pems/d04_text_station_raw_2016_01_04.txt.gz
83.6 M   pems/d04_text_station_raw_2016_01_05.txt.gz
...
```

I can move files to and from HDFS.
For example, to extract one file from HDFS to the Hadoop head node:

```{bash}
hdfs dfs -get pems/d04_text_station_raw_2016_01_01.txt.gz
```

Now I see it locally.

```{bash}
$ du -h d04_text_station_raw_2016_01_01.txt.gz
70M     d04_text_station_raw_2016_01_01.txt.gz
```

I can peek inside it.
It's stored in a gzip archive.

```{bash}
$ gunzip -c d04_text_station_raw_2016_01_01.txt.gz | head
```

In the end it's just a text file, a CSV.
It started compressed and stored in HDFS.


## Hive

Often our data looks more like tables than key value pairs.
In this case we can use one of the many SQL over Hadoop offerings, for example Apache Hive.


The file `d04_text_station_raw_2016_01_01.txt.gz` is part of a bigger table called `pems` that I've stored in Hive.
We can write SQL on this table.
Hive will transform the SQL into a MapReduce job and show us the progress.

```{bash}

$ hive
hive> SELECT COUNT(*) FROM pems;
```

There are 2.6 billion rows in this table, which is more than one order of magnitude larger than the usaspending data.


## R functions

Here are a couple features of the R language that I haven't yet had a chance to show you.

- __lazy evaluation__ allows us to write flexible function interfaces, which means we can call them in many different ways.
- __ellipses__ The `...` is used to pass arguments through from a wrapper function without having to name them.
    Question: Why would we want to do this?
    Because then we can pass any arguments at all, without having to specify them.
- __`on.exit`__ From the documentation: _`on.exit` records the expression given as its argument as needing to be executed when the current function exits (either naturally or as the result of an error).
This is useful for resetting graphical parameters or performing other cleanup actions._


```{r}
# Copy a table between databases in chunks.
chunk_copy_table = function(from_name
    , to_name = from_name
    , qstring = paste0("SELECT * FROM ", from_name)
    , .from_db = from_db
    , .to_db = to_db
    , chunk_size = 1e5L
    , ...
    )
{
    result = dbSendQuery(.from_db, qstring, ...)
    on.exit(dbClearResult(result))

    # Remaining code omitted
}
```
