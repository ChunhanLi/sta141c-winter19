Announcements:

- Next class meeting please bring your laptop for the mid quarter evaluation
- We're emailing you login credentials to the cluster. Don't try to change or customize them, because the configuration settings will just change them back.
- The next data set for the homework and final project is here: http://anson.ucdavis.edu/~clarkf/sta141c/ You can start thinking about what kind of questions and data analysis you would like to do.
- I'm working on the next homework- Next class meeting we'll learn about some more powerful ideas in bash that you'll apply to do it.

Cluster Resources:

- [SLURM documentation](https://slurm.schedmd.com/)
- [Gauss wiki](https://wiki.cse.ucdavis.edu/support/systems/gauss)
- [Introduction to Gauss slides](https://wiki.cse.ucdavis.edu/_media/support/systems/intro_to_gauss_slides.pdf) Paul Baines
- [Cluster monitoring](http://stats.cse.ucdavis.edu/ganglia/)
- email help@cse.ucdavis.edu to install software and get access if you're not in this class.


Clusters: gauss, peloton, NSF

I'm showing you one set of best practices.
There are other equally valid ways to do things.

Vocabulary:

- client: your laptop or desktop
- server: a computer that is not right in front of you
- node: a single computer
- head node: the computer that everyone logs into.
- hostname: a name that resolves to an IP address

Points:

1. workflow: write code locally, push it to Github, pull it to cluster
1. head node manages a queue of jobs and contention for resources
2. this class has a reserved partition. Use them, even if you can access everything.
3. interactive versus batch jobs
4. run your jobs on the worker nodes
5. job can either be waiting in the queue to start, running, or finished
6. network file system is an abstraction that makes all the files seem like they're on the same computer


The name resolves to an IP address:

```{bash}
nslookup gauss.cse.ucdavis.edu
```

login to gauss with username s141c-76:

```{bash}
ssh s141c-76@gauss.cse.ucdavis.edu
```

We can't see the letters appear.
Just trust and press enter.

No backups!
So use version control.

Verify that I am now interacting with a remote computer:

```{bash}
s141c-76@gauss:~$ whoami
s141c-76
s141c-76@gauss:~$ hostname
gauss.cse.ucdavis.edu
```

To logout, I just type `exit`.

```{bash}
exit
```

## moving code 

Put code that we've developed locally on the cluster.

```{bash}
$ git clone https://github.com/clarkfitzg/slurm-example.git
```

What software is available?

```{bash}
module avail
```

module is specific to cluster- easy way to synchronize


## running batch jobs

I have a script called `analysis.R` that I would like to run:

```{bash}
$ cd slurm-example/R
$ cat analysis.R
```

There's a secret message.

I also need a submission script

```{bash}
$ cat submit.sh
```

I can put it into the queue and have it run as follows:

```{bash}
sbatch ./submit.sh
```

It tells me that I submitted the job and brings me back to the head node.


## checking status

Suppose I do something dumb.
```{bash}
echo "
n = 0
while(n != Inf) n = n + 1
print(n)
" >> analysis.R

cat analysis.R
```

Question: Will `analysis.R` ever finish?
No, check in R: `1e100 == 1e100 + 1`.

Oh well, I'm just going to try it anyways.

```{bash}
sbatch ./submit.sh
```

Let's look at the queue and see who is running what:

```{bash}
$ squeue
```

online monitoring: http://stats.cse.ucdavis.edu/ganglia/

We can also examine the output file as it runs:

```{bash}
$ cat slurm-2065112.out
```

Oops, I need to cancel my job.

```
$ scancel --user=s141c-76
$ squeue
```

## array jobs

Could use a bunch of `sbatch` commands.

A better way is to use an array job.

```{bash}
cd ../Rarray

grep "array" submit.sh

cat submit.sh
```

Explain script.

Seeding the RNG in this way makes it reproducible.

```{bash}
$ Rscript analysis.R 3
```


## transferring data

Use git for small files < 1MB
Otherwise use `sftp` or `scp` from local machine.

```{bash}
clark@local $ sftp s141c-76@gauss.cse.ucdavis.edu
```

Commands:
- `ls` shows the files available in the current remote directory
- `cd` navigates to a different remote directory
- `get` downloads a file from the remote to the local
- `lls` shows the files available in the current remote directory
- `put` uploads a file from the local to the remote
- `exit` quits the sftp program


## interactive mode

Interactive mode on cluster is useful for developing and experimenting, not for large jobs.

```{bash}
srun --partition staclass --pty bash -i
```

Explain flags.
`bash` is the main argument, an executable to run

```{bash}
s141c-76@c0-10:~$ hostname
c0-10
```

`exit` brings me back to the head node.
