Possible topics:

- Bootstrap
- Reusable software in R / Python
- R, Numpy `stdin`
- Profiling
- Debugging in python with -m
- IO bound vs. CPU bound
- Splitting, map reduce
- error handling, better to ask forgiveness than to ask permission, look before you leap

Later:

- Numba compilation

## Announcements

- please don't request excessive resources on the cluster, because then others cannot use them.
  In particular, on the `staclass` partition please use 1 task at a time with up to two CPUs.  
  `#SBATCH --cpus-per-task=2`
- For the homework, It should take 1 or 2 hours to collect the counts of the first digits, and quite possibly less.


## Reading

- [Original bootstrap paper](https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41)
- [Bootstrap paper on massive data (little bag of bootstraps)](https://arxiv.org/abs/1112.5016)
- [Advanced R profiling chapter](http://adv-r.had.co.nz/Profiling.html)
- [Blog: 3 billion rows with R](http://clarkfitzg.github.io/2017/10/31/3-billion-rows-with-R/)


## bootstrap

This is an example of using the bootstrap.

The central limit theorem gives the distribution of the sample mean.
If the underlying distribution has standard deviation `sigma`, then the sample mean will have standard error `sigma / sqrt(n)`.
Let's check if the bootstrap sample mean has a similar standard error.

```{r}
set.seed(8390)
n = 1000L
x = rnorm(n)

sigma = 1
se_theory = sigma / sqrt(n)

b = 100L

# A single bootstrap sample:
x_star_1 = sample(x, size = n, replace = TRUE)

# They're not the same.
mean(x)
mean(x_star_1)

xbar_boot = replicate(b, mean(sample(x, size = n, replace = TRUE)))

se_boot = sd(xbar_boot)

se_theory
se_boot
```

They are reasonably close.

The idea is that the empirical distribution of the statistic on the bootstrap samples is close to the actual distribution of the statistic.

```{r}
hist(xbar_boot, freq = FALSE)

curve(dnorm(x, sd = se_theory), add = TRUE)
```

There are extensions of the bootstrap idea, see the reading list.


## Counting

Hopefully you're not writing a program to do counting.
There are many useful programs that will count for you.

From the command line, assuming that the data is sorted:

```{bash}
$ echo "a,1
a,1
a,1
a,2
a,2
b,1
b,1
b,5
b,5
b,5
b,5" > data.csv

$ uniq -c data.csv
```

We can feed this into R in a couple different ways.

Easiest is to just count everything with `table`.
This assumes that `data.csv` will fit in memory, which may not be the case with this data set.

```{r}
d = read.table("data.csv", sep = ",")
counts = table(d)
```

We can also pipe the output from the shell command.

```{r}
p = pipe("uniq -c data.csv")
counts = read.table(p)
```

This requires some subsequent massaging to get it into a form we can work with.

Python also has the ability to count things.

```{python}
from collections import Counter
import csv

f = open("data.csv")
rows = (tuple(r) for r in csv.reader(f))

c = Counter(rows)
```


## Monitoring Performance
