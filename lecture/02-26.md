Possible topics:

- Reusable software in R / Python
- R, Numpy `stdin`
- Profiling
- Debugging in python with -m
- IO bound vs. CPU bound
- Bootstrap
- Splitting, map reduce
- error handling, better to ask forgiveness than to ask permission, look before you leap

Later:

- Numba compilation

## Announcements

- please don't request excessive resources on the cluster, because then others cannot use them.
  In particular, on the `staclass` partition please use 1 task at a time with up to two CPUs.  
  `#SBATCH --cpus-per-task=2`
- For the homework, It should take 1 or 2 hours to collect the counts of the first digits, and quite possibly less.


## Reading

- [Advanced R profiling chapter](http://adv-r.had.co.nz/Profiling.html)
- [Blog: 3 billion rows with R](http://clarkfitzg.github.io/2017/10/31/3-billion-rows-with-R/)


## Counting

Hopefully you're not writing a program to do counting.
There are many useful programs that will count for you.

From the command line, assuming that the data is sorted:

```{bash}
$ echo "a,1
a,1
a,1
a,2
a,2
b,1
b,1
b,5
b,5
b,5
b,5" > data.csv

$ uniq -c data.csv
```

We can feed this into R in a couple different ways.

Easiest is to just count everything with `table`.
This assumes that `data.csv` will fit in memory, which may not be the case with this data set.

```{r}
d = read.table("data.csv", sep = ",")
counts = table(d)
```

We can also pipe the output from the shell command.

```{r}
p = pipe("uniq -c data.csv")
counts = read.table(p)
```

This requires some subsequent massaging to get it into a form we can work with.

Python also has the ability to count things.

```{python}
from collections import Counter
import csv

f = open("data.csv")
rows = (tuple(r) for r in csv.reader(f))

c = Counter(rows)
```


## bootstrap

```{python}

from random import choices

def boot(x, statistic):
    x_star = choices(x, k = len(x))
    return statistic(x_star)

```

