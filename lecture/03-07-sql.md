## Announcements

- Work on your projects.
    Besides posting on the usaspending website, the statistics department will also post a link on the department website.
    Want to go to graduate school or demonstrate competence to a potential employer?
    This is a great chance to get some visibility.
- Course evaluations open tomorrow.
    The University takes these seriously, especially for lecturers.
    A 4/5 average rating is an important threshold for the instructor.
    People will also read your free form comments.

Couple notes as you evaluate this class:

- My first time teaching, so I've learned plenty about what to do / not do.
- You gave me feedback mid quarter, and I implemented that.
- We've focused on real problems with real data.
    This is challenging, but the skills you've developed will serve you well.
- What other undergraduate class has given you the possibility of showcasing a project at a national website?


## Questions?


## Resources

Today we're going to talk about SQL and databases, because this was the most popular request for technologies to teach.
There's some great hands on learning material and theory out there on relational algebra.

- Nick Ulle and I taught an [introductory SQL workshop](http://anson.ucdavis.edu/~clarkf/sql/) last year.
- Codd's [seminal paper on databases](https://dl.acm.org/citation.cfm?id=362685)
- To learn at a deeper level [free online course from Stanford](https://lagunita.stanford.edu/courses/Home/Databases/Engineering/about)
- Good book: [designing data intensive applications](https://dataintensive.net/)
    I considered recommending this book for this course, it's a great read.


## Review

Last time we covered profiling.

1. Profile, see what parts of the program are slow.
2. Try to make the slow parts fast.
3. Repeat until it's fast enough.

In our case operating on many small data frames turned out to be slow.
We made it an order of magnitude faster by switching data structures, going from a data frame to a numeric vector.

Other ways to make code faster:
- find library where someone else has done it
- parallelism
- interface to compiled code


## Database Architecture

Term: __table__: Data consisting of rows and columns.
Examples include data frames in R and pandas, properly formatted spreadsheets.
In SQL, the rows and columns are _not_ ordered.

Question: What is a database? How is it different from a bunch of CSV files in a directory?
Question: Why use a database?
Answer: A bunch of CSV files in a directory is essentially a "poor man's database".
Real databases give you more:
- __schema__: database structure that defines what data can exist, and how tables relate to each other
- __Query__ Statement selecting data
- __SQL__ Structured Query Language
- __index__ an index on a column lets us efficiently extract rows based on that column


From Codd's seminal paper in 1970:

> Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation).
> ...
> Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed.
> Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information.
> ...
> A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced.

In the last few years 'tidy data' has become popular.
The idea is basically the same as it was when it was introduced 50 years ago.
It's a great idea.


## Actual Systems

Client / server. 
Typically many 

Databases are ubiquitious in large organizations.
If you go into a company as a data analyst / data scientist then you want access to as many databases as you can.
You'll spend lots of time writing ad hoc SQL queries, so you can learn it well.

__ad hoc query__ one off queries to answer specific questions

Two types of database:
- __transactional__ Whenever you make a bank transaction, book a plane ticket, buy an insurance policy, etc. some transactional system keeps a record.
    These kinds of databases are the lifeblood of many modern businesses, and they are treated very carefully.
- __data warehouse__ aka data lake, enterprise data warehouse.
    Typically data flows from one or more transactional systems into a data warehouse.
    Most entry data scientist jobs work with these.

SQLite is file based.
We're using it because it's simple, and it saves us spending a bunch of time on configuration.


## SQL

There are many popular databases out there.
Question: Which ones have you heard about?
    Oracle, Teradata, MySQL, Postgres, Hive

Almost all of them use some variation of SQL.
The variations tend to be pretty small.

Term: SQL is said to be a __declarative language__, which means we just describe what we want, and let the implementation figure out how to do it.

Here I'll connect with and use the database from the command line.

```{bash}
sqlite3 ~/data/usaspending.sqlite
```

It gives me an interactive SQL interpreter, which means I can start writing SQL.

```{sql}
SELECT toptier_agency_id, name FROM agency LIMIT 5;
```

This query gives me 5 arbitrary rows containing the values of the `toptier_agency_id`, `name` columns in the `agency` table.

Feel free to do this on the cluster.
`module load bio` to make `sqlite3` available.

To exit the SQLite interpreter:

```{sql}
.quit
```


## Example Queries

Simple query

```{sql}
SELECT award_id, awarding_agency_id, funding_amount
FROM `transaction`
WHERE 0 < funding_amount
LIMIT 10
;
```

Joins.
This is exactly the same idea as in the homework where we used lookup tables.

```{sql}
SELECT award_id, awarding_agency_id, funding_amount
FROM `transaction` as t
JOIN agency as a ON t.funding_agency_id = a.toptier_agency_id
LIMIT 10
;
```


## Integration with `stdout`

We can integrate SQL databases with bash processing pipelines.

Question: What do you think is easier, extracting from a database, or saving to a database?
Answer: Extracting is usually easier, because we don't have to worry about setting up a schema.

```{bash}
$ sqlite3 -header -csv ~/data/usaspending.sqlite "SELECT toptier_agency_id, name FROM agency LIMIT 20;" > first20.csv
```

If you're going to do this then you probably want to save the query into a separate file.

```{bash}
$ printf "SELECT toptier_agency_id, name FROM agency LIMIT 20;\n" > first20.sql
$ cat first20.sql | sqlite3 -header -csv ~/data/usaspending.sqlite > first20.csv

Check it has 20 rows plus a header = 21 rows:
$ wc -l first20.csv
```

What happened here is very cool.
The `sqlite3` command accepts a query through `stdin` and writes the output, a csv file, to `stdout`.


## Connecting to database from R

Programming languages can extract data directly from databases.
Question: Why might this approach be better than using an intermediate file?
Answer: Direct transfer is potentially more efficient, and there's no ambiguity about column types, in contrast to something lossy like CSV.

Most programming languages also have an abstraction layer that hides the details of the database.
Remember all the different vendors of databases?
The idea of the abstraction layer is that these databases are all similar, so we can write the same code in the host language (R), swap in different databases behind the scenes, and everything will work fine.
In practice, different combinations of languages and databases don't perfectly match this ideal.


```{r}
library(DBI)
library(RSQLite)

conn = dbConnect(SQLite(), "~/data/usaspending.sqlite")

# Read a query that we have saved in a different file.
query = readLines("first20.sql")

first20 = dbGetQuery(conn, query)

dbDisconnect(conn)
```


## Joins

[slides](https://docs.google.com/presentation/d/1mLDoKMtlt90U98jfB6A00nRuiMA4POXp6c_g7YORRMM/edit?usp=sharing)
