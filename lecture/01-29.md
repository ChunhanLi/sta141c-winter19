Possible Topics:

- memory profiling, when do objects actually get copied?
- "data mining" contrast with parametric statistics
- Clustering, cluster package
- agglomerative clustering, "cutting the tree", interpreting the results
- ways to think about machine learning models- generate data that does and does not satisfies the assumptions.
- interpreting clustering plots
- Extending objects by adding methods - but this is more useful if they're actually writing their own package.


Technique: A good way to understand a machine learning model, and the software that implements it, is to generate data that satisfies the assumptions of the model and check that the model gives you the results you expect.
Let's do that for clustering.

We'll start by generating 2 dimensional data that has 3 clusters around the points (0, 0), (1, 0), (0, 1).
We use 2 dimensions so that we can actually see the clusters.

```{r}
n = 100

# Makes this reproducible.
set.seed(489)
true_cluster = sample(1:3, size = n, replace = TRUE)

x = rep(0, n)
y = rep(0, n)

x[true_cluster == 2] = 1
y[true_cluster == 3] = 1

# Right now the points all overlap, so add some noise
x = x + rnorm(n, sd = 0.1)
y = y + rnorm(n, sd = 0.1)

plot(x, y)

# Better
plot(x, y, pch = true_cluster)
```

The goal of clustering is simple if vague: partition the data into groups.

`pam` is an acronym for Partitioning Around Medoids.

The basic idea: find k data points to be the medoids of the clusters, and assign all the other points to the closest medoid.
Do this in a way that minimizes the total distance between points and their assigned medoids.

A medoid is generalization of median- i.e. the data point that lies in the 'center' of the data, according to some measure of distance.

This is more robust to outliers than kmeans.


Let's make sure everyone is on the same page about what a distance calculation is.
We'll take the three two dimensional points that we used for our cluster centroids:

R's `dist` function computes the distance between each pair of points, where each row of data is considered a point.

```{r}
m = matrix(c(0, 1, 0, 0, 0, 1), nrow = 3)
m
```

The Euclidean (L2) distances between these points should be 1, 1, and sqrt(1 + 1).

```{r}
dist(m)
```

The Manhattan (L1) distances between these points should be 1, 1, and 1 + 1.

```{r}
dist(m, method = "manhattan")
```


## Reasoning about the computation:

pam is not computationally efficient for large data sets.
Computing a distance matrix can be expensive.
You have to compute pairwise distances between every pair of n choose 2 points.
For example, if you have 1 million 5 dimensional data points then you need to do at least 2.5 trillion calculations.
Computers can do this, it's not the end of the world.

```{r}
5 * choose(1e6, 2)
```

Question: How many possible cluster assignments are there for PAM with k clusters?
Answer: n choose k, one for each set of k medoids

So for the above cluster, if we're looking for 10 clusters:

```{r}
5 * choose(1e6, 10)
```

We're already up to 10^54 different options.
This is the end of the world pretty much- it's infeasible.

Pam deals with this by being a _greedy_ algorithm- it doesn't explore all possible options to find the very best.
All it tries to do is improve the objective function at every iteration.


## Playing around:

Now we'll play around with some algorithms.
I'm using the `cluster` package that comes as a recommended package with R.

```{r}
library(cluster)

# Modeling functions tend to play better with data frames.
xy = data.frame(x, y)

p1 = pam(xy, k = 3L)
```


Reading the documentation we see that the software saves the cluster assignments into `clustering`.

```{r}
mean(p1$clustering == true_cluster)
```

What, the clustering doesn't match the true clustering!
Question: What's going on?
Clusters only matter up to labeling.

```{r}
points(p1$medoids, bg = "blue", pch = 21)

```

Silhouettes are pretty cool, you can read about them.
The basic idea: each number represents how well each point fits into its cluster.

Reference: Silhouettes: A graphical aid to the interpretation and validation of cluster analysis by Peter J.Rousseeuw

## Agglomerative clustering

These clustering algorithms are based on "dissimilarites" between points, and this is what I asked you to work with on the homework.

If you don't pass them a 'distance' matrix, then they compute the distance between the points, where the points are rows in your data frame.

As I keep fitting these models, they keep computing the distance matrix.
No big deal for this small data, but if I spend a long time computing a distance then I would want to save this result.

Setting `trace.lev` gives me a little more information.

```{r}
a1 = agnes(xy, trace.lev=1)

plot(a1)
```

The dendrogram plot here provides some good evidence that there are three clusters.
We also have to keep in mind that this is essentially as good as it gets.

TODO: what is y axis?
TODO: what does the banner plot show?
